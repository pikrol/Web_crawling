{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web crawling experiments.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOcRitF04WyO7GlfDKreL3Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pikrol/Web_crawling/blob/main/Web_crawling_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anytree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J8RclmsskST",
        "outputId": "c0460764-86d9-4732-ed4a-cf33bcc6da9e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anytree\n",
            "  Downloading anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▉                        | 10 kB 22.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 20 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 30 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 40 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 41 kB 281 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from anytree) (1.15.0)\n",
            "Installing collected packages: anytree\n",
            "Successfully installed anytree-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-tjgo8yNit-C"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "from anytree import Node, RenderTree"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_website_structure(link, internal_only = True):\n",
        "\n",
        "  page = requests.get(link) #Get page HTML\n",
        "  page_soup = BeautifulSoup(page.text, 'html.parser') #Parse HTML\n",
        "  all_a = page_soup.findAll('a') #Find all 'a' tags\n",
        "  all_paths = list()\n",
        "\n",
        "  # (1) Prepare a list of all paths\n",
        "  for a in all_a:\n",
        "\n",
        "      if 'href' in a.attrs:\n",
        "\n",
        "        href = a.attrs[\"href\"] #Get href attribute value\n",
        "        href = href[len('https://'): ] #Remove https prefix\n",
        "        path = href.split('/') #Create list with subsequent pages\n",
        "        path = list(filter(None, path)) #Remove empty strings\n",
        "\n",
        "        #Prepare condition for storing the path\n",
        "        if internal_only:\n",
        "          append = path not in all_paths and len(path) > 0 and path[0] == link[len('https://'): ]\n",
        "        else:\n",
        "          append = path not in all_paths and len(path) > 0\n",
        "\n",
        "        #Store the path\n",
        "        if append: \n",
        "          all_paths.append(path)\n",
        "\n",
        "  # (2) Get pairs of subsequent pages\n",
        "  all_pairs = list()\n",
        "  for path in all_paths:\n",
        "    for i in range(len(path)):\n",
        "      if len(path) - 1 == i:\n",
        "        pair = [path[i]]\n",
        "      else:\n",
        "        pair = [path[i], path[i+1]]\n",
        "      if pair not in all_pairs: all_pairs.append(pair)\n",
        "\n",
        "  # (3) Prepare the tree and render it\n",
        "  def add_nodes(nodes, parent, child):\n",
        "      if parent not in nodes:\n",
        "          nodes[parent] = Node(parent)  \n",
        "      if child not in nodes:\n",
        "          nodes[child] = Node(child)\n",
        "      nodes[child].parent = nodes[parent]\n",
        "\n",
        "  #Prepare df from pairs\n",
        "  data = pd.DataFrame(columns=[\"Parent\",\"Child\"], data = all_pairs)\n",
        "\n",
        "  #Create all nodes\n",
        "  nodes = {} \n",
        "  data.apply(lambda x: add_nodes(nodes, x[\"Parent\"], x[\"Child\"]), axis=1)\n",
        "\n",
        "  #Find root\n",
        "  root = data[~data[\"Parent\"].isin(data[\"Child\"])][\"Parent\"].unique()[0] \n",
        "\n",
        "  #Print the tree\n",
        "  for pre, _, node in RenderTree(nodes[root]):\n",
        "      print(\"%s%s\" % (pre, node.name))"
      ],
      "metadata": {
        "id": "_L50p_YoAhVl"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_website_structure('https://multilada.pl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_8mszWycARs",
        "outputId": "b6e85a69-40d8-45fa-e053-905795aa0d87"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multilada.pl\n",
            "├── projekty\n",
            "│   ├── starwords\n",
            "│   ├── cdi\n",
            "│   ├── polkanorski\n",
            "│   └── clt\n",
            "├── zespol\n",
            "├── kontakt\n",
            "├── starwords-faq\n",
            "├── polityka-prywatnosci\n",
            "├── polityka-prywatnosci-polkanorski\n",
            "├── en\n",
            "└── no\n",
            "    └── None\n"
          ]
        }
      ]
    }
  ]
}